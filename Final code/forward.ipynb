{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 14)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('dataset.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "if df.isnull().sum().sum() > 0:\n",
    "    # Impute missing values with mean\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    df = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.94872647  0.68620244 -2.25177456  0.75752504 -0.2649003   2.394438\n",
      "  1.01668424  0.01719733 -0.69663055  1.08733806  2.27457861 -0.71113139\n",
      "  0.17622495]\n"
     ]
    }
   ],
   "source": [
    "# Scale numerical data (Standardization)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(X_scaled[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'SVM': SVC(kernel='linear'),  # Linear kernel to allow feature importance\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    # 'ANN': MLPClassifier(max_iter=1000)  # Simple ANN\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each model with Sequential Forward Selection\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False  True False  True False False False  True  True False  True\n",
      "  True]\n",
      "\n",
      "Model: Logistic Regression\n",
      "Optimal number of features: 6\n",
      "Selected Features: Index(['cp', 'chol', 'exang', 'oldpeak', 'ca', 'thal'], dtype='object')\n",
      "[False  True  True  True False False False  True False False False  True\n",
      "  True]\n",
      "\n",
      "Model: SVM\n",
      "Optimal number of features: 6\n",
      "Selected Features: Index(['sex', 'cp', 'trestbps', 'thalach', 'ca', 'thal'], dtype='object')\n",
      "[False  True  True False  True False False False  True  True  True False\n",
      " False]\n",
      "\n",
      "Model: KNN\n",
      "Optimal number of features: 6\n",
      "Selected Features: Index(['sex', 'cp', 'chol', 'exang', 'oldpeak', 'slope'], dtype='object')\n",
      "[False False  True False False False False False  True  True  True  True\n",
      "  True]\n",
      "\n",
      "Model: Naive Bayes\n",
      "Optimal number of features: 6\n",
      "Selected Features: Index(['cp', 'exang', 'oldpeak', 'slope', 'ca', 'thal'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Apply Forward Selection for each model\n",
    "for model_name, model in models.items():\n",
    "    sfs = SequentialFeatureSelector(model, direction='forward', scoring='accuracy', cv=5, n_features_to_select='auto')\n",
    "    sfs.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the selected features\n",
    "    selected_features = X.columns[sfs.get_support()]\n",
    "    print(sfs.get_support())\n",
    "    \n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"Optimal number of features: {len(selected_features)}\")\n",
    "    print(\"Selected Features:\", selected_features)\n",
    "    \n",
    "    X_train_selected = sfs.transform(X_train)\n",
    "    X_test_selected = sfs.transform(X_test)\n",
    "    \n",
    "    # Train the model on selected features\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_selected)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='binary')\n",
    "    recall = recall_score(y_test, y_pred, average='binary')\n",
    "    f1 = f1_score(y_test, y_pred, average='binary')\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Selected Features': list(selected_features),\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "    })\n",
    "    \n",
    "    # Print evaluation metrics\n",
    "    # print(f\"Selected Features: {selected_features}\")\n",
    "    # print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    # print(f\"Precision: {precision:.4f}\")\n",
    "    # print(f\"Recall: {recall:.4f}\")\n",
    "    # print(f\"F1 Score: {f1:.4f} \\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of Model Performance:\n",
      "\n",
      "Model: Logistic Regression\n",
      "Selected Features: ['cp', 'chol', 'exang', 'oldpeak', 'ca', 'thal']\n",
      "Accuracy: 0.8852\n",
      "Precision: 0.9032\n",
      "Recall: 0.8750\n",
      "F1 Score: 0.8889\n",
      "\n",
      "Model: SVM\n",
      "Selected Features: ['sex', 'cp', 'trestbps', 'thalach', 'ca', 'thal']\n",
      "Accuracy: 0.9180\n",
      "Precision: 0.8857\n",
      "Recall: 0.9688\n",
      "F1 Score: 0.9254\n",
      "\n",
      "Model: KNN\n",
      "Selected Features: ['sex', 'cp', 'chol', 'exang', 'oldpeak', 'slope']\n",
      "Accuracy: 0.7541\n",
      "Precision: 0.7742\n",
      "Recall: 0.7500\n",
      "F1 Score: 0.7619\n",
      "\n",
      "Model: Naive Bayes\n",
      "Selected Features: ['cp', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n",
      "Accuracy: 0.8525\n",
      "Precision: 0.8710\n",
      "Recall: 0.8438\n",
      "F1 Score: 0.8571\n"
     ]
    }
   ],
   "source": [
    "# Show the results\n",
    "print(\"\\nSummary of Model Performance:\")\n",
    "for result in results:\n",
    "    print(f\"\\nModel: {result['Model']}\")\n",
    "    print(f\"Selected Features: {result['Selected Features']}\")\n",
    "    print(f\"Accuracy: {result['Accuracy']:.4f}\")\n",
    "    print(f\"Precision: {result['Precision']:.4f}\")\n",
    "    print(f\"Recall: {result['Recall']:.4f}\")\n",
    "    print(f\"F1 Score: {result['F1 Score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import BaggingClassifier\n",
    "# # Apply Bagging for each of the models evaluated earlier\n",
    "# for model_name, model in models.items():\n",
    "#     print(f\"\\nApplying Bagging on Model: {model_name}\")\n",
    "    \n",
    "#     # Perform Forward Selection (already done above)\n",
    "#     sfs = SequentialFeatureSelector(model, direction='forward', scoring='accuracy', cv=5, n_features_to_select='auto')\n",
    "#     sfs.fit(X_train, y_train)\n",
    "    \n",
    "#     # Get the selected features\n",
    "#     selected_features = X.columns[sfs.get_support()]\n",
    "#     X_train_selected = sfs.transform(X_train)\n",
    "#     X_test_selected = sfs.transform(X_test)\n",
    "    \n",
    "#     # Apply Bagging on the model\n",
    "#     bagging_model = BaggingClassifier(estimator=model, n_estimators=50, random_state=42)\n",
    "#     bagging_model.fit(X_train_selected, y_train)\n",
    "    \n",
    "#     # Make predictions with Bagging\n",
    "#     y_pred_bagging = bagging_model.predict(X_test_selected)\n",
    "    \n",
    "#     # Evaluate the Bagging model\n",
    "#     accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
    "#     precision_bagging = precision_score(y_test, y_pred_bagging, average='binary')\n",
    "#     recall_bagging = recall_score(y_test, y_pred_bagging, average='binary')\n",
    "#     f1_bagging = f1_score(y_test, y_pred_bagging, average='binary')\n",
    "    \n",
    "#     # Print Bagging evaluation metrics\n",
    "#     print(f\"Selected Features: {selected_features}\")\n",
    "#     print(f\"Bagging Accuracy: {accuracy_bagging:.4f}\")\n",
    "#     print(f\"Bagging Precision: {precision_bagging:.4f}\")\n",
    "#     print(f\"Bagging Recall: {recall_bagging:.4f}\")\n",
    "#     print(f\"Bagging F1 Score: {f1_bagging:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import StackingClassifier\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "# # Define the base models and the final model for stacking\n",
    "# base_models = [\n",
    "#     ('svm', SVC(probability=True)),\n",
    "#     ('naive_bayes', GaussianNB()),\n",
    "#     ('logistic_regression', LogisticRegression()),\n",
    "#     ('ann', MLPClassifier(max_iter=1000))\n",
    "# ]\n",
    "\n",
    "# # Final model to combine the base models' predictions\n",
    "# final_model = LogisticRegression()\n",
    "\n",
    "# # Create the StackingClassifier\n",
    "# stacking_model = StackingClassifier(estimators=base_models, final_estimator=final_model, cv=5)\n",
    "\n",
    "# # Fit the stacking model with the selected features\n",
    "# stacking_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# # Make predictions with the stacking model\n",
    "# y_pred_stacking = stacking_model.predict(X_test_selected)\n",
    "\n",
    "# # Evaluate the stacking model\n",
    "# accuracy_stacking = accuracy_score(y_test, y_pred_stacking)\n",
    "# precision_stacking = precision_score(y_test, y_pred_stacking, average='binary')\n",
    "# recall_stacking = recall_score(y_test, y_pred_stacking, average='binary')\n",
    "# f1_stacking = f1_score(y_test, y_pred_stacking, average='binary')\n",
    "\n",
    "# # Print Stacking model evaluation metrics\n",
    "# print(\"\\nStacking Model Evaluation Metrics:\")\n",
    "# print(f\"Stacking Accuracy: {accuracy_stacking:.4f}\")\n",
    "# print(f\"Stacking Precision: {precision_stacking:.4f}\")\n",
    "# print(f\"Stacking Recall: {recall_stacking:.4f}\")\n",
    "# print(f\"Stacking F1 Score: {f1_stacking:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import VotingClassifier\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "# # Define the base models for voting\n",
    "# base_models_voting = [\n",
    "#     ('svm', SVC(probability=True)),\n",
    "#     ('naive_bayes', GaussianNB()),\n",
    "#     ('logistic_regression', LogisticRegression()),\n",
    "#     ('ann', MLPClassifier(max_iter=1000))\n",
    "# ]\n",
    "\n",
    "# # Create the VotingClassifier\n",
    "# voting_model = VotingClassifier(estimators=base_models_voting, voting='soft')\n",
    "\n",
    "# # Fit the voting model with the selected features\n",
    "# voting_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# # Make predictions with the voting model\n",
    "# y_pred_voting = voting_model.predict(X_test_selected)\n",
    "\n",
    "# # Evaluate the voting model\n",
    "# accuracy_voting = accuracy_score(y_test, y_pred_voting)\n",
    "# precision_voting = precision_score(y_test, y_pred_voting, average='binary')\n",
    "# recall_voting = recall_score(y_test, y_pred_voting, average='binary')\n",
    "# f1_voting = f1_score(y_test, y_pred_voting, average='binary')\n",
    "\n",
    "# # Print Voting model evaluation metrics\n",
    "# print(\"\\nVoting Model Evaluation Metrics:\")\n",
    "# print(f\"Voting Accuracy: {accuracy_voting:.4f}\")\n",
    "# print(f\"Voting Precision: {precision_voting:.4f}\")\n",
    "# print(f\"Voting Recall: {recall_voting:.4f}\")\n",
    "# print(f\"Voting F1 Score: {f1_voting:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create a function to evaluate each model with AdaBoost\n",
    "def evaluate_boosting_model(base_model, model_name):\n",
    "    # Create an AdaBoost classifier with a base model\n",
    "    boosting_model = AdaBoostClassifier(estimator=base_model, n_estimators=50, random_state=42)\n",
    "\n",
    "    # Fit the boosting model with the selected features\n",
    "    boosting_model.fit(X_train_selected, y_train)\n",
    "\n",
    "    # Make predictions with the boosting model\n",
    "    y_pred_boosting = boosting_model.predict(X_test_selected)\n",
    "\n",
    "    # Evaluate the boosting model\n",
    "    accuracy_boosting = accuracy_score(y_test, y_pred_boosting)\n",
    "    precision_boosting = precision_score(y_test, y_pred_boosting, average='binary')\n",
    "    recall_boosting = recall_score(y_test, y_pred_boosting, average='binary')\n",
    "    f1_boosting = f1_score(y_test, y_pred_boosting, average='binary')\n",
    "\n",
    "    # Print Boosting model evaluation metrics\n",
    "    print(f\"\\n{model_name} Boosting Model Evaluation Metrics:\")\n",
    "    print(f\"Boosting Accuracy: {accuracy_boosting:.4f}\")\n",
    "    print(f\"Boosting Precision: {precision_boosting:.4f}\")\n",
    "    print(f\"Boosting Recall: {recall_boosting:.4f}\")\n",
    "    print(f\"Boosting F1 Score: {f1_boosting:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of base models to evaluate with AdaBoost\n",
    "base_models_to_boost = [\n",
    "    (SVC(probability=True), 'SVM'),\n",
    "    (GaussianNB(), 'Naive Bayes'),\n",
    "    (LogisticRegression(), 'Logistic Regression'),\n",
    "    # (MLPClassifier(max_iter=1000), 'ANN'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM Boosting Model Evaluation Metrics:\n",
      "Boosting Accuracy: 0.8033\n",
      "Boosting Precision: 0.8846\n",
      "Boosting Recall: 0.7188\n",
      "Boosting F1 Score: 0.7931\n",
      "\n",
      "Naive Bayes Boosting Model Evaluation Metrics:\n",
      "Boosting Accuracy: 0.4754\n",
      "Boosting Precision: 0.0000\n",
      "Boosting Recall: 0.0000\n",
      "Boosting F1 Score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Program Files\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Boosting Model Evaluation Metrics:\n",
      "Boosting Accuracy: 0.9016\n",
      "Boosting Precision: 0.9062\n",
      "Boosting Recall: 0.9062\n",
      "Boosting F1 Score: 0.9062\n"
     ]
    }
   ],
   "source": [
    "# Evaluate each model with AdaBoost\n",
    "for model_class, model_name in base_models_to_boost:\n",
    "    # Create an instance of the model\n",
    "    # model_instance = model_class()  # Instantiate the model here\n",
    "    evaluate_boosting_model(model_class, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Program Files\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Variables to store the best model and score\n",
    "best_model = None\n",
    "best_f1_score = 0\n",
    "\n",
    "# Evaluate each model with AdaBoost and keep track of the best one\n",
    "for model_class, model_name in base_models_to_boost:\n",
    "    model_instance = model_class\n",
    "    boosting_model = AdaBoostClassifier(estimator=model_instance, n_estimators=50, random_state=42)\n",
    "    \n",
    "    # Train and evaluate the model\n",
    "    boosting_model.fit(X_train_selected, y_train)\n",
    "    y_pred_boosting = boosting_model.predict(X_test_selected)\n",
    "    f1_boosting = f1_score(y_test, y_pred_boosting, average='binary')\n",
    "    \n",
    "    # Save the model if it's the best one so far\n",
    "    if f1_boosting > best_f1_score:\n",
    "        best_f1_score = f1_boosting\n",
    "        best_model = boosting_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best AdaBoost model has been saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save the best AdaBoost model\n",
    "import joblib\n",
    "joblib.dump(best_model, 'best_ada_model.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(sfs, 'sfs.pkl')\n",
    "print(\"Best AdaBoost model has been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
